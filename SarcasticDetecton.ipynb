{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "59ada132-6e59-41c7-90f0-f1e2220e1c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/ifti/anaconda3/lib/python3.9/site-packages (3.6.5)\n",
      "Requirement already satisfied: click in /home/ifti/anaconda3/lib/python3.9/site-packages (from nltk) (8.0.3)\n",
      "Requirement already satisfied: joblib in /home/ifti/anaconda3/lib/python3.9/site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/ifti/anaconda3/lib/python3.9/site-packages (from nltk) (2021.8.3)\n",
      "Requirement already satisfied: tqdm in /home/ifti/anaconda3/lib/python3.9/site-packages (from nltk) (4.62.3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import nltk\n",
    "# !pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dad09d2c-2a5d-4a98-b7df-8bc9cb36762a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     class                                               text\n",
      "0  notsarc  If that's true, then Freedom of Speech is doom...\n",
      "1  notsarc  Neener neener - is it time to go in from the p...\n",
      "2  notsarc  Just like the plastic gun fear, the armour pie...\n",
      "3  notsarc  So geology is a religion because we weren't he...\n",
      "4  notsarc  Well done Monty. Mark that up as your first ev...\n",
      "5  notsarc  But the whole process was done in the courts u...\n",
      "6  notsarc  so we would penalize financially those who wan...\n",
      "7  notsarc  So, IOW, you are saying that those of us who a...\n",
      "8  notsarc  your probably referring to the mexican basin c...\n",
      "9  notsarc  Not in matters of atheism. Tell me, when scien...\n"
     ]
    }
   ],
   "source": [
    "def Gathering_data():\n",
    "    \n",
    "    \n",
    "    # file path nor sarcastic datasets\n",
    "    not_sarcastic_d1 = \"Datset2/Sample/GEN-sarc-notsarc.csv\"\n",
    "    not_sarcastic_d2 = \"Datset2/Sample/HYP-sarc-notsarc.csv\"\n",
    "    not_sarcastic_d3 = \"Datset2/Sample/RQ-sarc-notsarc.csv\"\n",
    "    \n",
    "    # Reading each Dataframe\n",
    "    not_sar_d1_df = pd.read_csv(not_sarcastic_d1)\n",
    "    not_sar_d2_df = pd.read_csv(not_sarcastic_d2)\n",
    "    not_sar_d3_df = pd.read_csv(not_sarcastic_d3)\n",
    "    \n",
    "    #drop extra column from each dataframe\n",
    "    not_sar_d1_df.drop(columns=['id'],inplace=True)\n",
    "    not_sar_d2_df.drop(columns=['id'],inplace=True)\n",
    "    not_sar_d3_df.drop(columns=['id'],inplace=True)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # labeling the non sarcastic datasets for non sarcastic datasets\n",
    "    \n",
    "    non_sarcastic_dic = {\n",
    "    'text' : [],\n",
    "    'class':[]\n",
    "        \n",
    "    }\n",
    "    \n",
    "    \n",
    "    dir_path = os.listdir(\"Notebook/Datasets/Dataset1/notsarc\")\n",
    "    # print(dir_path)\n",
    "    \n",
    "    for file_name in dir_path:\n",
    "        try:\n",
    "\n",
    "            file = open(\"Notebook/Datasets/Dataset1/notsarc/\"+file_name,\"r+\") \n",
    "            # dumpoing text of each file in the dictionary\n",
    "            non_sarcastic_dic[\"text\"].append(file.read())\n",
    "            non_sarcastic_dic[\"class\"].append('notsarc')\n",
    "            \n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    # turning dic to dataframe\n",
    "    not_sar_d4_df = pd.DataFrame.from_dict(non_sarcastic_dic)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # labeling the sarcastic datasets for non sarcastic datasets\n",
    "    \n",
    "    sarcastic_dic = {\n",
    "    'text' : [],\n",
    "    'class':[]\n",
    "        \n",
    "    }\n",
    "    \n",
    "    \n",
    "    dir_path = os.listdir(\"Notebook/Datasets/Dataset1/sarc\")\n",
    "    # print(dir_path)\n",
    "    \n",
    "    for file_name in dir_path:\n",
    "        try:\n",
    "\n",
    "            file = open(\"Notebook/Datasets/Dataset1/sarc/\"+file_name,\"r+\") \n",
    "            # dumpoing text of each file in the dictionary\n",
    "            non_sarcastic_dic[\"text\"].append(file.read())\n",
    "            non_sarcastic_dic[\"class\"].append('sarc')\n",
    "            \n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    # turning dic to dataframe\n",
    "    sar_d5_df = pd.DataFrame.from_dict(sarcastic_dic)\n",
    "    \n",
    "            \n",
    "    \n",
    "    frames = [not_sar_d1_df,\n",
    "              not_sar_d2_df,\n",
    "              not_sar_d3_df,\n",
    "              not_sar_d4_df,\n",
    "             sar_d5_df]\n",
    "    \n",
    "    \n",
    "    \n",
    "    # combine frames \n",
    "    not_sarcastic_df = pd.concat(frames)\n",
    "\n",
    "     \n",
    "    not_sarcastic_df.to_csv(\"pre-processed_datasets/pre-processed_sarcastic_data.csv\")\n",
    "    \n",
    "def Preprocessing():\n",
    "    \n",
    "    # file path \n",
    "    \n",
    "    filePath = \"pre-processed_datasets/pre-processed_sarcastic_data.csv\"\n",
    "    \n",
    "    df = pd.read_csv(filePath)\n",
    "    df.drop(columns=['Unnamed: 0'],inplace=True)\n",
    "\n",
    "    # head\n",
    "    print(df.head(10))\n",
    "    \n",
    "    # Tokenization\n",
    "\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    #Gathering_data()\n",
    "    Preprocessing()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fca7c12-b151-4710-946a-912f4010e563",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364c8e93-9d34-41be-af6a-164585835547",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b958b4d4-c759-43f6-9334-988b6e406904",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80d609d-bda6-4f39-9be0-293647f73eae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35ef1c5-5c17-4641-8cec-4c58e8922c48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "529e9d79-6316-483c-975f-ea16283b7d74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c6ac1712-ac18-4e21-a4ba-52f34e7926c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6d9f27b7-4be2-44e7-82b5-eb13f85b9ad5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39251, 2)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3165a4d8-73ee-4db7-b680-51699ffcb9cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a46b50-d8bd-427e-a9e6-3a0c154569dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
