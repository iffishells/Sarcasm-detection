{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "59ada132-6e59-41c7-90f0-f1e2220e1c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ifti/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/ifti/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ifti/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "# import these modules\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "\n",
    "# !pip install nltk\n",
    "from numba import jit ,cuda\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "dad09d2c-2a5d-4a98-b7df-8bc9cb36762a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed stopward character : \n",
      " ['true', 'freedom', 'speech', 'doom', 'harass', 'is', 'subject', 'claim', 'that', 'book', 'i', 'dont', 'like', 'is', 'harass', 'ban']    notsarc\n",
      "Removed stopward character : \n",
      " ['neener', 'neener', 'time', 'go', 'playground', 'yet']    notsarc\n",
      "Removed stopward character : \n",
      " ['like', 'plastic', 'gun', 'fear', 'the', 'armour', 'pierc', 'bullet', 'fear', 'wa', 'built', 'upon', 'misinform', 'myth']    notsarc\n",
      "Removed stopward character : \n",
      " ['geolog', 'religion', 'becaus', 'werent', 'see', 'rock', 'x', 'form']    notsarc\n",
      "Removed stopward character : \n",
      " ['well', 'done', 'monti', 'mark', 'first', 'ever', 'honest', 'accur', 'post']    notsarc\n",
      "Removed stopward character : \n",
      " ['whole', 'process', 'wa', 'done', 'the', 'court', 'use', 'the', 'process', 'set', 'the', 'constitut', 'the', 'law', 'clear', 'issu', 'in', 'massachusett', 'the', 'court', 'job', 'decid', 'the', 'court', 'are', 'a', 'check', 'on', 'the', 'injustic', 'toward', 'minor', 'by', 'the', 'major']    notsarc\n",
      "Removed stopward character : \n",
      " ['would', 'penal', 'financi', 'want', 'kid']    notsarc\n",
      "Removed stopward character : \n",
      " ['iow', 'say', 'u', 'are', 'intellig', 'enough', 'realiz', 'that', 'non', 'exist', 'imaginari', 'god', 'doesnt', 'exist', 'chemic', 'imbal', 'wow', 'that', 'quit', 'a', 'stretch', 'even', 'you', 'archi', 'babi']    notsarc\n",
      "Removed stopward character : \n",
      " ['probabl', 'refer', 'mexican', 'basin', 'crater', 'mass', 'extinct', 'doesnt', 'prove', 'evolut', 'the', 'flood', 'describ', 'the', 'bibl', 'exampl', 'mass', 'extinct', 'besid', 'in', 'order', 'the', 'asteroid', 'theori', 'to', 'work', 'the', 'asteroid', 'would', 'to', 'creat', 'an', 'almost', 'global', 'tidal', 'wave', 'would', 'effect', 'kill', 'the', 'anim', 'drown', 'also', 'similiar', 'to', 'the', 'effect', 'of', 'the', 'flood']    notsarc\n",
      "Removed stopward character : \n",
      " ['matter', 'atheism', 'tell', 'scienc', 'discov', 'someth', 'seem', 'agre', 'year', 'old', 'write', 'of', 'darwin', 'the', 'write', 'of', 'darwin', 'disregard', 'that', 'type', 'of', 'histori', 'trumpet', 'to', 'the', 'mass']    notsarc\n",
      "Removed stopward character : \n",
      " ['front', 'behind', 'leaf']    notsarc\n",
      "Removed stopward character : \n",
      " ['realli', 'dont', 'think', 'know', 'you', 'talk', 'plea', 'provid', 'evid', 'back', 'claim', 'thank']    notsarc\n",
      "     class                                               text\n",
      "0  notsarc  [true, freedom, speech, doom, harass, is, subj...\n",
      "1  notsarc        [neener, neener, time, go, playground, yet]\n",
      "2  notsarc  [like, plastic, gun, fear, the, armour, pierc,...\n",
      "3  notsarc  [geolog, religion, becaus, werent, see, rock, ...\n",
      "4  notsarc  [well, done, monti, mark, first, ever, honest,...\n",
      "5  notsarc  [whole, process, wa, done, the, court, use, th...\n",
      "6  notsarc                 [would, penal, financi, want, kid]\n",
      "7  notsarc  [iow, say, u, are, intellig, enough, realiz, t...\n",
      "8  notsarc  [probabl, refer, mexican, basin, crater, mass,...\n",
      "9  notsarc  [matter, atheism, tell, scienc, discov, someth...\n"
     ]
    }
   ],
   "source": [
    "#@jit\n",
    "def Gathering_data():\n",
    "    \n",
    "    \n",
    "    # file path nor sarcastic datasets\n",
    "    not_sarcastic_d1 = \"Datset2/Sample/GEN-sarc-notsarc.csv\"\n",
    "    not_sarcastic_d2 = \"Datset2/Sample/HYP-sarc-notsarc.csv\"\n",
    "    not_sarcastic_d3 = \"Datset2/Sample/RQ-sarc-notsarc.csv\"\n",
    "    \n",
    "    # Reading each Dataframe\n",
    "    not_sar_d1_df = pd.read_csv(not_sarcastic_d1)\n",
    "    not_sar_d2_df = pd.read_csv(not_sarcastic_d2)\n",
    "    not_sar_d3_df = pd.read_csv(not_sarcastic_d3)\n",
    "    \n",
    "    #drop extra column from each dataframe\n",
    "    not_sar_d1_df.drop(columns=['id'],inplace=True)\n",
    "    not_sar_d2_df.drop(columns=['id'],inplace=True)\n",
    "    not_sar_d3_df.drop(columns=['id'],inplace=True)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # labeling the non sarcastic datasets for non sarcastic datasets\n",
    "    \n",
    "    non_sarcastic_dic = {\n",
    "    'text' : [],\n",
    "    'class':[]\n",
    "        \n",
    "    }\n",
    "    \n",
    "    \n",
    "    dir_path = os.listdir(\"Notebook/Datasets/Dataset1/notsarc\")\n",
    "    # print(dir_path)\n",
    "    \n",
    "    for file_name in dir_path:\n",
    "        try:\n",
    "\n",
    "            file = open(\"Notebook/Datasets/Dataset1/notsarc/\"+file_name,\"r+\") \n",
    "            # dumpoing text of each file in the dictionary\n",
    "            non_sarcastic_dic[\"text\"].append(file.read())\n",
    "            non_sarcastic_dic[\"class\"].append('notsarc')\n",
    "            \n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    # turning dic to dataframe\n",
    "    not_sar_d4_df = pd.DataFrame.from_dict(non_sarcastic_dic)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # labeling the sarcastic datasets for non sarcastic datasets\n",
    "    \n",
    "    sarcastic_dic = {\n",
    "    'text' : [],\n",
    "    'class':[]\n",
    "        \n",
    "    }\n",
    "    \n",
    "    \n",
    "    dir_path = os.listdir(\"Notebook/Datasets/Dataset1/sarc\")\n",
    "    # print(dir_path)\n",
    "    \n",
    "    for file_name in dir_path:\n",
    "        try:\n",
    "\n",
    "            file = open(\"Notebook/Datasets/Dataset1/sarc/\"+file_name,\"r+\") \n",
    "            # dumpoing text of each file in the dictionary\n",
    "            non_sarcastic_dic[\"text\"].append(file.read())\n",
    "            non_sarcastic_dic[\"class\"].append('sarc')\n",
    "            \n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    # turning dic to dataframe\n",
    "    sar_d5_df = pd.DataFrame.from_dict(sarcastic_dic)\n",
    "    \n",
    "            \n",
    "    \n",
    "    frames = [not_sar_d1_df,\n",
    "              not_sar_d2_df,\n",
    "              not_sar_d3_df,\n",
    "              not_sar_d4_df,\n",
    "             sar_d5_df]\n",
    "    \n",
    "    \n",
    "    \n",
    "    # combine frames \n",
    "    not_sarcastic_df = pd.concat(frames)\n",
    "\n",
    "     \n",
    "    not_sarcastic_df.to_csv(\"pre-processed_datasets/pre-processed_sarcastic_data.csv\")\n",
    "    \n",
    "def Preprocessing():\n",
    "    \n",
    "    # file path \n",
    "    \n",
    "    filePath = \"pre-processed_datasets/pre-processed_sarcastic_data.csv\"\n",
    "    \n",
    "    df = pd.read_csv(filePath)\n",
    "    df.drop(columns=['Unnamed: 0'],inplace=True)\n",
    "\n",
    "    # Tokenization\n",
    "    for index , text  in df.iterrows():\n",
    "        \n",
    "        # case 0 : converting into the lower case:\n",
    "        text[1]  = text[1].lower()\n",
    "        # print(\"Convering to lower case : \\n\",text[1])\n",
    "        \n",
    "        # cleaing remove unseless character or special character\n",
    "        # case 1 : remove extra and white spaces from the string\n",
    "        # print(type(text[1]))\n",
    "        \n",
    "        text[1] = text[1].strip()\n",
    "        text[1] = re.sub(\"[0-9!(),~@#.$%^&*/'()_=?-]*\", '', text[1])\n",
    "        \n",
    "        \n",
    "        # print(\"Removed special character : \\n\",text[1])\n",
    "        \n",
    "        stopword = stopwords.words('english')\n",
    "\n",
    "        \n",
    "        text[1] = word_tokenize(text[1])\n",
    "        \n",
    "        # print(\"Tokenization : \\n\" ,text[1]) \n",
    "        \n",
    "        # case 3 : stemming \n",
    "           \n",
    "        ps = PorterStemmer()\n",
    "        # case 4 : lemmitization : \n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        \n",
    "\n",
    "    \n",
    "        for w in range(0,len(text[1])):\n",
    "            text[1][w] = ps.stem(text[1][w])\n",
    "            text[1][w] = lemmatizer.lemmatize(text[1][w])\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        # case 2: remove the stopwards\n",
    "        for sptword in stopword:\n",
    "            if sptword in text[1]:\n",
    "                text[1].remove(sptword)\n",
    "        \n",
    "        print(\"Removed stopward character : \\n\",text[1] , \"  \",text[0])\n",
    "        \n",
    "        if index >10:\n",
    "            break\n",
    "        \n",
    "        \n",
    "        \n",
    "    # head\n",
    "    print(df.head(10))\n",
    "    \n",
    "    df.to_csv(\"pre-processed_datasets/pre-processing_text.csv\",index=False)\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    #Gathering_data()\n",
    "    Preprocessing()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9fca7c12-b151-4710-946a-912f4010e563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['if', 'that', 'true', 'then', 'freedom', 'of', 'speech', 'is', 'doom', 'harass', 'is', 'subject', 'now', 'i', 'can', 'claim', 'that', 'a', 'book', 'i', 'dont', 'like', 'is', 'harass', 'me', 'and', 'it', 'will', 'be', 'ban']\n"
     ]
    }
   ],
   "source": [
    "hel = ['if', 'thats', 'true', 'then', 'freedom', 'of', 'speech', 'is', 'doomed', 'harassment', 'is', 'subjective', 'now', 'i', 'can', 'claim', 'that', 'a', 'book', 'i', 'dont', 'like', 'is', 'harassing', 'me', 'and', 'it', 'will', 'be', 'banned']\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "    \n",
    "for w in range(0,len(hel)):\n",
    "    hel[w] = ps.stem(hel[w])\n",
    "    \n",
    "print(hel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "364c8e93-9d34-41be-af6a-164585835547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "l1 =  ['if', 'that', 'true', 'then', 'freedom', 'of', 'speech', 'is', 'doom', 'harass', 'is', 'subject', 'now', 'i', 'can', 'claim', 'that', 'a', 'book', 'i', 'dont', 'like', 'is', 'harass', 'me', 'and', 'it', 'will', 'be', 'ban']\n",
    "l2 =  ['if', 'that', 'true', 'then', 'freedom', 'of', 'speech', 'is', 'doom', 'harass', 'is', 'subject', 'now', 'i', 'can', 'claim', 'that', 'a', 'book', 'i', 'dont', 'like', 'is', 'harass', 'me', 'and', 'it', 'will', 'be', 'ban']\n",
    "print(l1==l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b958b4d4-c759-43f6-9334-988b6e406904",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - '/home/ifti/nltk_data'\n    - '/home/ifti/anaconda3/nltk_data'\n    - '/home/ifti/anaconda3/share/nltk_data'\n    - '/home/ifti/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.subdir}/{zip_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet.zip/wordnet/\u001b[0m\n\n  Searched in:\n    - '/home/ifti/nltk_data'\n    - '/home/ifti/anaconda3/nltk_data'\n    - '/home/ifti/anaconda3/share/nltk_data'\n    - '/home/ifti/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_51960/3675226350.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlemmatizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"rocks :\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlemmatizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"rocks\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"corpora :\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlemmatizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"corpora\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/nltk/stem/wordnet.py\u001b[0m in \u001b[0;36mlemmatize\u001b[0;34m(self, word, pos)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mlemma\u001b[0m \u001b[0mof\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mword\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mgiven\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \"\"\"\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mlemmas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_morphy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemmas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlemmas\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.subdir}/{zip_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;31m# Load the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                 \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.subdir}/{self.__name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - '/home/ifti/nltk_data'\n    - '/home/ifti/anaconda3/nltk_data'\n    - '/home/ifti/anaconda3/share/nltk_data'\n    - '/home/ifti/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# import these modules\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(\"rocks :\", lemmatizer.lemmatize(\"rocks\"))\n",
    "print(\"corpora :\", lemmatizer.lemmatize(\"corpora\"))\n",
    "\n",
    "# a denotes adjective in \"pos\"\n",
    "print(\"better :\", lemmatizer.lemmatize(\"better\", pos =\"a\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80d609d-bda6-4f39-9be0-293647f73eae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35ef1c5-5c17-4641-8cec-4c58e8922c48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "529e9d79-6316-483c-975f-ea16283b7d74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c6ac1712-ac18-4e21-a4ba-52f34e7926c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6d9f27b7-4be2-44e7-82b5-eb13f85b9ad5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39251, 2)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3165a4d8-73ee-4db7-b680-51699ffcb9cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a46b50-d8bd-427e-a9e6-3a0c154569dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
